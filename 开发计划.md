# 本地推理编排服务（Local Runtime）开发计划（冻结口径）

## 0. 实现状态
- 已落地（Phase 0 进行中）：
  - C++20 + CMake 工程骨架（跨平台）
  - HTTP Server（cpp-httplib）+ OpenAI Router 基础路由
  - OpenAI 风格 endpoints：`/v1/models`、`/v1/chat/completions`、`/v1/embeddings`、`/v1/responses`
  - ChatCompletions SSE（`data: {json}\n\n` + `data: [DONE]\n\n`）
  - 会话/turn 骨架：`session_id`、`turn_id`（兼容无状态调用；可选 `use_server_history`）
  - Ollama Provider：models（`/api/tags`）、chat（`/api/chat`）、embeddings（`/api/embeddings`）
- 已落地（Phase 1 进行中：Tool Calling）：
  - Tool Registry：内置工具 `runtime.add` / `runtime.echo` / `runtime.time`
  - Tool Loop：模型输出 tool_calls → Server 执行 → 注入 TOOL_RESULT → 继续生成 → 产出 final
  - SSE tool_calls：支持 `delta.tool_calls[].function.arguments` 的分片输出（增量拼接形态）
  - 可验证用例：`model=fake-tool` 可稳定触发一次 tool_call 并返回最终答案
- 已落地（Phase 2 进行中：MCP 工具注入）：
  - MCP Client（HTTP JSON-RPC）：`initialize` / `tools/list` / `tools/call`
  - MCP 工具映射：启动时从 MCP Server 拉取 tools 并注册为可调用 Tool
  - MCP 刷新：提供 `/internal/refresh_mcp_tools` 触发重新拉取 tools 并动态注册
  - 调用护栏：MCP Client 支持 per-server 并发上限（in-flight）与可配置超时
  - 可验证用例：通过 `MCP_HOST` 注入 `mcp.echo`，并在 Tool Loop 中成功调用并回填
- 已落地（Phase 3 进行中：LSP Facade via MCP）：
  - 多 MCP Server：支持 `MCP_HOSTS` 逗号分隔注入多个 MCP Server；遇到工具名冲突时自动暴露为 `mcpN.<name>`
  - LSP Mock：提供 `lsp.hover` / `lsp.definition` 的 MCP Mock Server，用于端到端验证 LSP Facade 的工具形态与回填链路
  - Facade 工具：提供 `ide.read_file` / `ide.search` / `ide.diagnostics` / `ide.hover` / `ide.definition`
  - 工作区约束：支持 `RUNTIME_WORKSPACE_ROOT`，对文件路径/URI 越界访问进行拦截
- 已落地（Phase 4 进行中：Planner / 安全护栏 / 可观测性）：
  - Planner（最小闭环）：可选 `planner=true`（或对象参数）触发“先产出 plan，再按 plan 执行 tool，最后汇总输出”
  - Planner v1：plan allowlist+schema 校验与 `planner.max_rewrites` 重写机制，避免无效计划反复执行
  - Tool Loop 护栏：支持 `max_steps` / `max_tool_calls` 限制，避免无限 tool 循环
  - Tool allowlist：严格校验模型请求的 tool 是否在本次 request 的 tools 列表内，不允许则回填错误结果
  - Trace 输出：可选 `trace=true`，通过 `x-runtime-trace` header 输出执行链路（plan/steps/tool_calls/tool_results）
- 已知限制（待后续迭代解决）：
  - 上游（Ollama）真实流式：cpp-httplib Client 当前不提供 POST 的增量 content receiver，现阶段以“非流式上游 + 服务端分片模拟 SSE”满足 opencode 的 SSE 依赖；若要 token 级实时，需要引入支持流式 POST 的上游客户端实现
  - Tool Calling 的“强约束 JSON 输出”依赖提示词与解析器；对真实本地模型（Ollama）效果仍需在回归集上验证与迭代
  - `/v1/responses`：当前仅实现最小非流式子集（后续按未决项冻结字段与与 ChatCompletions 的映射）
  - MCP 传输层：当前按“单 HTTP POST JSON-RPC（request/response）”实现最小可用；尚未支持 Streamable HTTP 的 SSE 通知/流式与 stdio transport
  - LSP：当前仅打通“通过 MCP 暴露 LSP 工具”的通路与验证用例；尚未内置真实 LSP Client/Server 管理、工作区与增量文档同步

## 1. 问题定义
在本地推理后端（首期：Ollama）之上，构建一个跨平台（Windows/macOS/Linux）的 C++20 HTTP 推理编排服务。对外提供兼容 OpenAI API 的 HTTP REST + SSE Streaming 接口（兼容 opencode 的 OpenAI Router 调用方式），对内通过 Provider 抽象对接不同模型后端，并在服务端完成 Tool Calling 执行闭环，逐步增强本地模型在工具使用（Tool Calling / MCP / LSP / Planner）上的能力与稳定性。

## 2. 目标
- 协议兼容：提供 OpenAI 风格 endpoints 与字段语义，兼容 opencode 依赖的流式输出。
- 流式体验：SSE 使用 OpenAI ChatCompletions SSE 形态，必须支持 tool_calls.arguments 的增量拼接。
- 会话强能力：采用混合会话语义（B + C）——Client 可控会话标识与策略参数，Server 主导工具执行与多步编排状态机。
- 工具闭环：Tool Calling 必须由 Server 执行，工具结果回填模型形成闭环。
- 路线图：Tool Calling → MCP → LSP → Planner（后续：Memory、JSON 强约束、多 Agent、长任务、安全控制）。

## 3. 约束
- 语言/工程：C++20；CMake 跨平台工程结构。
- 依赖建议：cpp-httplib（HTTP server/client）；nlohmann/json（JSON）。
- HTTP：REST API + SSE streaming（ChatCompletions SSE 形态）。
- 跨平台：Windows / macOS / Linux。

## 4. OpenAI API 兼容范围（冻结）
- 必须支持的 endpoints：
  - `POST /v1/chat/completions`
  - `GET /v1/models`
  - `POST /v1/embeddings`
  - `POST /v1/responses`
- SSE（冻结）：
  - 采用 OpenAI ChatCompletions SSE 形态（`data: {json}\n\n` + `data: [DONE]\n\n`）
  - 必须支持 tool_calls.arguments 的增量拼接（流式返回片段 → 服务端聚合为完整可解析 arguments）

## 5. 会话语义（冻结）
- 混合模型（B + C）：
  - Client 可控：`session_id`（可选提供/可由 Server 分配）、部分策略参数（例如预算/上限/工具白名单等，具体在实现时冻结）
  - Server 主导：turn 状态机、工具执行闭环、多步编排推进、失败恢复与清理
- 关键标识建议：
  - `session_id`：会话级
  - `turn_id`：一次请求/一次编排轮次级
  - `tool_call_id`：一次工具调用级
  - `request_id`：HTTP 请求级（用于观测与排障）

## 6. 组件与责任边界（冻结）
### 6.1 HTTP 接入层（OpenAI API Router）
- 路由 `/v1/*` endpoints
- 请求规范化为内部 Canonical Request
- SSE 编码与连接生命周期管理
- 错误格式化为 OpenAI 风格响应

### 6.2 会话状态管理（Session Manager）
- 会话创建/续传、turn 状态机
- 流式增量缓冲与最终结果关联
- 工具调用上下文管理（增量参数聚合、执行状态、回填状态）

### 6.3 编排内核（Orchestrator）
- 单次请求闭环：生成 → 解析 tool_calls → 执行工具 → 回填 → 继续生成
- 预留 Planner 扩展点（分步执行/自纠错/Workflow）

### 6.4 Tool Calling 子系统（Tool Registry + Executor）
- 工具注册：name、JSON schema、权限级别、超时、并发/预算
- 参数聚合：支持 SSE 增量片段拼接、最终 JSON 校验
- 服务端执行：执行结果结构化回传并回填模型

### 6.5 MCP 连接器（MCP Client）
- Runtime 作为 MCP Client：连接 MCP Tool Server、发现工具、调用工具
- 将 MCP 工具映射为 Runtime 内部 Tool Registry 中的可调用工具

### 6.6 LSP 能力外观层（LSP Facade，基于 MCP 工具实现）
- 拓扑冻结：`IDE ⇄ LSP ⇄ Runtime ⇄ 模型`
- 原则冻结：模型不能直接操作 LSP；只能调用 Runtime 提供的高层工具（Facade）
- 能力范围：文件系统 + AST + 编辑 + 跳转 + refactor（以 Facade 工具形式暴露）

### 6.7 Provider 抽象层（Model Provider Interface）
- 统一对接不同推理后端（首期：Ollama）
- 统一流式回调与取消/超时语义（便于 SSE 与编排内核复用）

### 6.8 观测与回归评测
- 关键指标：tool_call 成功率、平均回填轮数、平均重试次数、端到端耗时、SSE 断流率
- 链路追踪：按 `session_id/turn_id/tool_call_id/request_id` 关联事件

## 7. Phase 规划（冻结顺序：Tool Calling → MCP → LSP → Planner）
### Phase 0：协议骨架 + SSE + 会话最小闭环
- 输出：四个 endpoints 可用；ChatCompletions SSE 可用；会话/turn 状态可串联并可观测
- 验收：opencode 可稳定连接与消费 SSE，不丢 token/不断流/不乱序

### Phase 1：Tool Calling（服务端执行）+ SSE 增量参数拼接
- 输出：工具注册/执行/回填闭环；对 tool_calls.arguments 支持增量拼接并最终可解析
- 验收：固定用例集中 tool_call 解析与执行成功率显著提升；失败可结构化回传并受控重试

### Phase 2：MCP（工具注入）
- 输出：MCP Client + 工具发现/调用；MCP 工具映射为模型可调用工具
- 验收：可刷新工具清单；MCP 调用有超时、并发上限与失败隔离

### Phase 3：LSP（通过 MCP Tool Server）
- 输出：稳定的 LSP Facade 工具集（读取/编辑/跳转/引用/重构等）
- 验收：覆盖至少 5 类核心能力闭环；模型不接触 LSP 协议细节；不破坏 IDE/LSP 一致性

### Phase 4：Planner（分步执行 / 自纠错 / Workflow）
- 输出：Plan 表示、逐步执行引擎、校验与重试、可复用 workflow 模板
- 验收：多步任务完成率提升；错误不无限循环（最大步数/预算控制）；中断可恢复或安全终止

## 8. 风险点（冻结）
- SSE tool_calls 增量拼接复杂：需要 turn 内维护“进行中的 tool_call 缓冲”，避免 JSON 断裂与多调用交织。
- `/v1/responses` 与 `/v1/chat/completions` 语义差异：建议内部统一 Canonical 数据结构，再分别做协议转换。
- 混合会话（B+C）复杂度高：必须明确 client 可控与 server 主导的边界与幂等策略。
- LSP 权限与一致性：通过 IDE 侧（MCP Tool Server）驱动 LSP，避免 Runtime 成为第二个 LSP client 争抢状态。
- 工具执行安全：即使安全控制在后期，首期也需最小防护（工具 allowlist、超时、并发限制、工作区根目录约束）。

## 9. 未决项（进入实现前按需冻结）
- `/v1/responses` 兼容的字段子集与与 ChatCompletions 的映射规则。
- 会话持久化策略：内存起步或可插拔持久化（影响长任务与恢复）。
- MCP Tool Server 的传输层协议与部署形态（HTTP/WebSocket/stdio 等）。
- Ollama 的 chat/embeddings/streaming/取消请求语义如何映射到统一 Provider 接口。

## 10. 测试与回归（建议）
- 一键回归：`python3 tools/regression.py --workspace-root /Users/acproject/workspace/cpp_projects/local-ai-runtime`
- 手动冒烟：`bash tools/smoke.sh http://127.0.0.1:8080`
- 常用环境变量：
  - `MCP_HOSTS=http://127.0.0.1:9001/`
  - `RUNTIME_WORKSPACE_ROOT=/Users/acproject/workspace/cpp_projects/local-ai-runtime`
  - （可选）`RUNTIME_LISTEN_HOST=127.0.0.1` `RUNTIME_LISTEN_PORT=8080`
  - Provider 选择：
    - 默认 Provider：`RUNTIME_PROVIDER=llama_cpp|ollama|mnn|lmdeploy`
    - llama.cpp 模型路径：`LLAMA_CPP_MODEL=/abs/path/to/model.gguf`
    - 外部 Provider endpoint：`OLLAMA_HOST=http://127.0.0.1:11434`、`MNN_HOST=http://127.0.0.1:8000`、`LMDEPLOY_HOST=http://127.0.0.1:23333`
    - model 前缀路由：`model="ollama:<id>"` / `model="mnn:<id>"` / `model="lmdeploy:<id>"` / `model="llama_cpp:<id>"`
- 验收关注点：
  - `/internal/refresh_mcp_tools` 返回 `ok=true` 且 `registered` 符合预期
  - `trace=true` 时 `x-runtime-trace` 包含 `plan/plan_rewrites/plan_steps/tool_calls/tool_results`
